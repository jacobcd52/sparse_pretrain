_wandb:
    value:
        cli_version: 0.23.1
        e:
            v1t97eggd4u4s3yoa0uciir06bq4re1k:
                args:
                    - --config
                    - configs/default.yaml
                cpu_count: 112
                cpu_count_logical: 224
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "322122547200"
                        used: "3430744064"
                email: jcdguitarguy@googlemail.com
                executable: /usr/local/bin/python
                git:
                    commit: 611955a26ee6ba3a22537df398b79476f0e40203
                    remote: https://github.com/jacobcd52/sparse_pretrain.git
                gpu: NVIDIA B200
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Blackwell
                      cudaCores: 18944
                      memoryTotal: "192265846784"
                      name: NVIDIA B200
                      uuid: GPU-6af3c2ad-8c5d-5bb6-a737-cbc663367e33
                host: 157cc880a6ac
                memory:
                    total: "1622818902016"
                os: Linux-6.8.0-87-generic-x86_64-with-glibc2.39
                program: -m src.train
                python: CPython 3.12.3
                root: /root/sparse_pretrain
                startedAt: "2025-12-13T04:33:41.935214Z"
                writerId: v1t97eggd4u4s3yoa0uciir06bq4re1k
        m: []
        python_version: 3.12.3
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
            "3":
                - 3
                - 16
            "4": 3.12.3
            "5": 0.23.1
            "6": 4.57.3
            "12": 0.23.1
            "13": linux-x86_64
model:
    value:
        activation: gelu
        d_head: 16
        d_mlp: 4096
        d_model: 1024
        dropout: 0
        n_ctx: 256
        n_layer: 8
        tie_embeddings: false
        use_attention_sinks: true
        use_bias: true
        use_bigram_table: true
        use_flash_attention: true
        use_positional_embeddings: false
        use_rms_norm: true
        vocab_size: null
optimizer:
    value:
        beta1: 0.9
        beta2: 0.95
        enable_grad_clip: true
        enable_lr_decay: true
        eps: 0.1
        grad_clip_rms: 1
        learning_rate: 0.0128
        use_sharkfin_schedule: true
        warmup_fraction: 0.01
        weight_decay: 0.1
sparsity:
    value:
        activation_sparsity_locations: attn_in,attn_out,mlp_in,mlp_out,mlp_neuron,attn_v,attn_k,attn_q
        activation_topk_fraction: 0.25
        enable_activation_sparsity: true
        enable_weight_sparsity: true
        min_weights_per_neuron: 4
        sparsity_anneal_end_fraction: 0.5
        sparsity_anneal_start_fraction: 0.01
        target_l0_fraction: 0.015625
training:
    value:
        batch_size: 64
        checkpoint_dir: checkpoints
        checkpoint_every_n_steps: 1000
        dataset_name: ""
        dataset_split: train
        eval_every_n_steps: 20
        gradient_accumulation_steps: 1
        gradient_checkpointing: true
        keep_n_checkpoints: 5
        log_every_n_steps: 1
        log_gradients_every_n_steps: 10
        log_sparsity_every_n_steps: 50
        log_weights_every_n_steps: 100
        mixed_precision: bf16
        seed: 0
        text_column: text
        tokenizer_name: ""
        total_tokens: 100000000
        use_wandb: true
        wandb_entity: null
        wandb_project: circuit_sparsity
        wandb_run_name: null
