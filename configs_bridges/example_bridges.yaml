# Example configuration for bridge training
# Based on Gao et al. (2025) "Weight-sparse transformers have interpretable circuits"
#
# This trains a weight-sparse model coupled to a frozen dense model via bridges.
# The dense model must be a SparseGPT model with no sparsity enabled.

# ============================================================================
# Frozen Dense Model
# ============================================================================
dense_model:
  # HuggingFace repo ID or local path to the pretrained dense model
  # This model must have been trained with enable_weight_sparsity=false
  # and enable_activation_sparsity=false
  repo_id: "username/my-dense-model"

# ============================================================================
# Sparse Model Architecture (trained from scratch)
# ============================================================================
sparse_model:
  # The sparse model can have different dimensions than the dense model
  # Bridges will handle the mapping between different d_model sizes
  
  n_layer: 4           # Should match dense model's n_layer
  d_model: 512         # Can differ from dense model
  n_ctx: 512           # Context length
  d_head: 16           # Head dimension (typically same as dense)
  d_mlp: null          # Defaults to 4 * d_model
  
  # Architecture settings (typically match dense model)
  use_rms_norm: true
  tie_embeddings: false
  use_positional_embeddings: false
  use_bigram_table: false
  use_attention_sinks: true
  activation: gelu
  dropout: 0.0
  use_bias: true
  use_flash_attention: true

# ============================================================================
# Bridge Settings
# ============================================================================
bridges:
  # AbsTopK fraction for bridge encoders
  # k = encoder_afrac * d_sparse for the AbsTopK in each encoder
  encoder_afrac: 0.25
  
  # Loss coefficients (all configurable)
  # Total loss = sum of (coef * loss_component)
  
  # Cross-entropy on sparse model's logits vs actual next tokens
  coef_ce_sparse: 1.0
  
  # KL divergence: KL(dense_logits || sparse_logits)
  # Distillation from dense to sparse
  coef_kl_sparse: 1.0
  
  # Normalized MSE reconstruction loss for bridges
  # sum over sites of: NMSE(encoder(h_d), h_s) + NMSE(decoder(h_s), h_d)
  coef_nmse: 1.0
  
  # KL for dense→sparse hybrid passes
  # At each site, encode dense activation and run sparse model to end
  coef_kl_d2s: 1.0
  
  # KL for sparse→dense hybrid passes
  # At each site, decode sparse activation and run dense model to end
  coef_kl_s2d: 1.0

# ============================================================================
# Sparsity Settings (for sparse model, NOT bridges)
# ============================================================================
sparsity:
  # Weight sparsity
  enable_weight_sparsity: true
  target_l0_fraction: 0.015625  # 1/64, paper default
  
  # Sparsity annealing schedule
  sparsity_anneal_start_fraction: 0.01
  sparsity_anneal_end_fraction: 0.5
  anneal_type: linear
  
  # Minimum nonzero weights per neuron
  min_weights_per_neuron: 4
  
  # Activation sparsity
  enable_activation_sparsity: true
  activation_topk_fraction: 0.25
  activation_sparsity_locations: "attn_in,attn_out,mlp_in,mlp_out,mlp_neuron,attn_v,attn_k,attn_q"

# ============================================================================
# Optimizer Settings
# ============================================================================
optimizer:
  optimizer_type: adamw
  learning_rate: 0.001
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.1
  eps: 0.1  # Unusually large epsilon (paper recommendation)
  
  enable_grad_clip: true
  grad_clip_rms: 1.0
  
  warmup_fraction: 0.01
  enable_lr_decay: true
  use_sharkfin_schedule: false

# ============================================================================
# Training Settings
# ============================================================================
training:
  # Dataset
  dataset_name: "SimpleStories/SimpleStories"
  dataset_split: train
  text_column: story
  
  # Tokenizer
  tokenizer_name: "SimpleStories/SimpleStories-1.25M"
  
  # Training duration
  total_tokens: 1000000000  # 1B tokens
  batch_size: 64
  gradient_accumulation_steps: 1
  
  # Mixed precision
  mixed_precision: bf16
  
  # Checkpointing
  checkpoint_dir: checkpoints/bridges_example
  checkpoint_every_n_steps: 1000
  keep_n_checkpoints: 5
  
  # Logging
  log_every_n_steps: 10
  log_gradients_every_n_steps: 100
  log_weights_every_n_steps: 100
  log_sparsity_every_n_steps: 100
  eval_every_n_steps: 100
  
  # Validation
  val_split: test
  val_holdout_fraction: 0.01
  val_max_batches: 20
  
  # W&B
  wandb_project: bridges_training
  wandb_run_name: bridges_example
  wandb_entity: null
  use_wandb: true
  
  # Reproducibility
  seed: 0
  
  # HuggingFace Hub (optional)
  hf_repo: null  # Set to "username/model-name" to upload

